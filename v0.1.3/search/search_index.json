{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Template Python project","text":"<p>This is a template Python project which can be used to bootstrap a new library in the Pasqal quantum software codebase.</p>"},{"location":"#development-tools","title":"Development tools","text":"<p>The library uses the following tools:</p> <ul> <li>hatch for managing virtual environment and dependencies</li> <li>pytest for building the unit tests suite</li> <li>black, isort and flake8 for code formatting and linting</li> <li>mypy for static type checking</li> <li>pre-commit for applying linting and formatting automatically before committing new code</li> </ul> <p>We recommend to use <code>pyenv</code> for managing python versions for managing python versions both globally and locally: <pre><code># System-wide install of a python version.\npyenv install 3.10\n\n# Use 3.10 everywhere.\npyenv global 3.10\n\n# Or locally in the current directory.\npyenv local 3.10\n</code></pre></p>"},{"location":"#install-from-registry","title":"Install from registry","text":"<p>Before you can install the library from the private Pasqal PyPi, make sure to ask for <code>PYPI_USERNAME</code> and <code>PYPI_PASSWORD</code> on the relevant Slack channel. You can then set the credentials as environment variables via:</p> <pre><code>export PYPI_USERNAME=MYUSERNAME\nexport PYPI_PASSWORD=THEPASSWORD\n</code></pre> <p>You are then able to install the latest version of <code>template-python-project</code> from the Pasqal private PyPi.</p>"},{"location":"#install-from-source","title":"Install from source","text":"<p>All Pasqal quantum libraries require Python &gt;=3.8. For development, the preferred method to install this package is to use <code>hatch</code>. You can install from source by cloning this repository and run:</p> <pre><code>python -m pip install hatch\npython -m hatch -v shell\n\n# execute any script using the library\npython my_script.py\n</code></pre> <p>Alternatively, you can also:</p> <ul> <li>install with <code>pip</code> in development mode by simply running <code>pip install -e .</code>. Notice that in this way   you will install all the dependencies, including extras.</li> <li>install it with <code>conda</code> by simply using <code>pip</code> inside the Conda environment.</li> </ul>"},{"location":"#develop","title":"Develop","text":"<p>When developing the package, the recommended way is to create a virtual environment with <code>hatch</code> as shown above:</p> <pre><code>python -m pip install hatch\npython -m hatch -v shell\n</code></pre> <p>When inside the shell with development dependencies, install first the pre-commit hook: <pre><code>pre-commit install\n</code></pre></p> <p>In this way, you will get automatic linting and formatting every time you commit new code. Do not forget to run the unit test suite by simply running the <code>pytest</code> command.</p> <p>If you do not want to get into the Hatch shell, you can alternatively do the following:</p> <pre><code>python -m pip install hatch\npython -m hatch -v shell\n\n# install the pre-commit\npython -m hatch run pre-commit install\n\n# commit some code\npython -m hatch run git commit -m \"My awesome commit\"\n\n# run the unit tests suite\npython -m hatch run pytest\n</code></pre>"},{"location":"#document","title":"Document","text":"<p>You can improve the documentation of the package by editing this file for the landing page or adding new markdown or Jupyter notebooks to the <code>docs/</code> folder in the root of the project. In order to modify the table of contents, edit the <code>mkdocs.yml</code> file in the root of the project.</p> <p>In order to build and serve the documentation locally, you can use <code>hatch</code> with the right environment:</p> <pre><code>python -m hatch -v run docs:build\npython -m hatch -v run docs:serve\n</code></pre> <p>If you don't want to use <code>hatch</code>, just check into your favorite virtual environment and execute the following commands:</p> <pre><code>python -m pip install -r docs/requirements.txt\nmkdocs build\nmkdocs serve\n</code></pre>"},{"location":"sample_page/","title":"Sample page","text":"<p>This is just a sample notebook to showcase the rendering of Jupyter notebooks in the documentation.</p> <pre><code>msg = \"To be udpated\"\nprint(msg)\n</code></pre>   To be udpated"},{"location":"qinfo_tools/qng/","title":"The Quantum Natural Gradient optimizer","text":"<p>Qadence-libs provides a set of optimizers based on quantum information tools, in particular based on the Quantum Fisher Information<sup>1</sup> (QFI). The Quantum Natural Gradient <sup>2</sup> (QNG) is a gradient-based optimizer which uses the QFI matrix to better navigate the optimizer's descent to the minimum. The parameter update rule for the QNG optimizer is written as:</p> \\[ \\theta_{t+1} = \\theta_t - \\eta g^{-1}(\\theta_t)\\nabla \\mathcal{L}(\\theta_t) \\] <p>where \\(g(\\theta)\\) is the Fubiny-Study metric tensor (aka Quantum Geometric Tensor), which is equivalent to the Quantum Fisher Information matrix \\(F(\\theta)\\) up to a constant factor \\(F(\\theta)= 4 g(\\theta)\\). The Quantum Fisher Information can be written as the Hessian of the fidelity of a quantum state:</p> \\[   F_{i j}(\\theta)=-\\left.2 \\frac{\\partial}{\\partial \\theta_i} \\frac{\\partial}{\\partial \\theta_j}\\left|\\left\\langle\\psi\\left(\\theta^{\\prime}\\right) \\mid \\psi(\\theta)\\right\\rangle\\right|^2\\right|_{{\\theta}^{\\prime}=\\theta} \\] <p>However, computing the above expression is a costly operation scaling quadratically with the number of parameters in the variational quantum circuit. It is thus usual to use approximate methods when dealing with the QFI matrix. Qadence-Libs provides a SPSA-based implementation of the Quantum Natural Gradient<sup>3</sup>. The SPSA (Simultaneous Perturbation Stochastic Approximation) algorithm is a well known finite differences-based algorithm. QNG-SPSA constructs an iterative approximation to the QFI matrix with a constant number of circuit evaluations that does not scale with the number of parameters. Although the SPSA algorithm outputs a rough approximation of the QFI matrix, the QNG-SPSA has been proven to work well while being a very efficient method due to the constant overhead in circuit evaluations (only 6 extra evaluations per iteration).</p> <p>In this tutorial, we use the QNG and QNG-SPSA optimizers with the Quantum Circuit Learning algorithm, a variational quantum algorithm which uses Quantum Neural Networks as universal function approximators.</p> <p>Keep in mind that only circuit parameters can be optimized with the QNG optimizer, since we can only calculate the QFI matrix of parameters contained in the circuit. If your model holds other trainable, non-circuit parameters, such as scaling or shifting of the input/output, another optimizer must be used for to optimize those parameters. <pre><code>import torch\nfrom torch.utils.data import random_split\nimport random\nimport matplotlib.pyplot as plt\n\nfrom qadence import QuantumCircuit, QNN, FeatureParameter\nfrom qadence import kron, tag, hea, RX, Z, hamiltonian_factory\n\nfrom qadence_libs.qinfo_tools import QuantumNaturalGradient\nfrom qadence_libs.types import FisherApproximation\n</code></pre> </p> <p>First, we prepare the Quantum Circuit Learning data. In this case we will fit a simple one-dimensional sin(\\(x\\)) function: <pre><code># Ensure reproducibility\nseed = 0\ntorch.manual_seed(seed)\nrandom.seed(seed)\n\n# Create dataset\ndef qcl_training_data(\n    domain: tuple = (0, 2 * torch.pi), n_points: int = 200\n) -&gt; tuple[torch.Tensor, torch.Tensor]:\n    start, end = domain\n\n    x_rand, _ = torch.sort(torch.DoubleTensor(n_points).uniform_(start, end))\n    y_rand = torch.sin(x_rand)\n\n    return x_rand, y_rand\n\n\nx, y = qcl_training_data()\n\n# random train/test split of the dataset\ntrain_subset, test_subset = random_split(x, [0.75, 0.25])\ntrain_ind = sorted(train_subset.indices)\ntest_ind = sorted(test_subset.indices)\n\nx_train, y_train = x[train_ind], y[train_ind]\nx_test, y_test = x[test_ind], y[test_ind]\n</code></pre> </p> <p>We now create the base Quantum Circuit that we will use with all the optimizers: <pre><code>n_qubits = 3\n\n# create a simple feature map to encode the input data\nfeature_param = FeatureParameter(\"phi\")\nfeature_map = kron(RX(i, feature_param) for i in range(n_qubits))\nfeature_map = tag(feature_map, \"feature_map\")\n\n# create a digital-analog variational ansatz using Qadence convenience constructors\nansatz = hea(n_qubits, depth=n_qubits)\nansatz = tag(ansatz, \"ansatz\")\n\n# Observable\nobservable = hamiltonian_factory(n_qubits, detuning= Z)\n</code></pre> </p>"},{"location":"qinfo_tools/qng/#optimizers","title":"Optimizers","text":"<p>We will experiment with three different optimizers: ADAM, QNG and QNG-SPSA. To train a model with the different optimizers we will create a <code>QuantumModel</code> and reset the values of their variational parameters before each training loop so that all of them have the same starting point.</p> <pre><code># Build circuit and model\ncircuit = QuantumCircuit(n_qubits, feature_map, ansatz)\nmodel = QNN(circuit, [observable])\n\n# Loss function\nmse_loss = torch.nn.MSELoss()\n\n# Initial parameter values\ninitial_params = torch.rand(model.num_vparams)\n</code></pre> <p>We can now train the model with the different corresponding optimizers:</p>"},{"location":"qinfo_tools/qng/#adam","title":"ADAM","text":"<pre><code># Train with ADAM\nn_epochs_adam = 20\nlr_adam = 0.1\n\nmodel.reset_vparams(initial_params)\noptimizer = torch.optim.Adam(model.parameters(), lr=lr_adam)\n\nloss_adam = []\nfor i in range(n_epochs_adam):\n    optimizer.zero_grad()\n    loss = mse_loss(model(values=x_train).squeeze(), y_train.squeeze())\n    loss_adam.append(float(loss))\n    loss.backward()\n    optimizer.step()\n</code></pre>"},{"location":"qinfo_tools/qng/#qng","title":"QNG","text":"<p>The way to initialize the <code>QuantumNaturalGradient</code> optimizer in <code>qadence-libs</code> is slightly different from other usual Torch optimizers. Normally, one needs to pass a <code>params</code> argument to the optimizer to specify which parameters of the model should be optimized. In the <code>QuantumNaturalGradient</code>, it is assumed that all circuit parameters are to be optimized, whereas the non-circuit parameters will not be optimized. By circuit parameters, we mean parameters that somehow affect the quantum gates of the circuit and therefore influence the final quantum state. Any parameters affecting the observable (such as ouput scaling or shifting) are not considered circuit parameters, as those parameters will not be included in the QFI matrix as they don't affect the final state of the circuit.</p> <p>The <code>QuantumNaturalGradient</code> constructor takes a qadence's <code>QuantumModel</code> as the 'model', and it will automatically identify its circuit and non-circuit parameters. The <code>approximation</code> argument defaults to the SPSA method, however the exact version of the QNG is also implemented and can be used for small circuits (beware of using the exact version for large circuits, as it scales badly). \\(\\beta\\) is a small constant added to the QFI matrix before inversion to ensure numerical stability,</p> \\[(F_{ij} + \\beta \\mathbb{I})^{-1}\\] <p>where \\(\\mathbb{I}\\) is the identify matrix. It is always a good idea to try out different values of \\(\\beta\\) if the training is not converging, which might be due to a too small \\(\\beta\\).</p> <pre><code># Train with QNG\nn_epochs_qng = 20\nlr_qng = 0.1\n\nmodel.reset_vparams(initial_params)\noptimizer = QuantumNaturalGradient(\n    model=model,\n    lr=lr_qng,\n    approximation=FisherApproximation.EXACT,\n    beta=0.1,\n)\n\nloss_qng = []\nfor i in range(n_epochs_qng):\n    optimizer.zero_grad()\n    loss = mse_loss(model(values=x_train).squeeze(), y_train.squeeze())\n    loss_qng.append(float(loss))\n    loss.backward()\n    optimizer.step()\n</code></pre>"},{"location":"qinfo_tools/qng/#qng-spsa","title":"QNG-SPSA","text":"<p>The QNG-SPSA optimizer can be constructed similarly to the exact QNG, where now a new argument \\(\\epsilon\\) is used to control the shift used in the finite differences derivatives of the SPSA algorithm.</p> <pre><code># Train with QNG-SPSA\nn_epochs_qng_spsa = 20\nlr_qng_spsa = 0.01\n\nmodel.reset_vparams(initial_params)\noptimizer = QuantumNaturalGradient(\n    model=model,\n    lr=lr_qng_spsa,\n    approximation=FisherApproximation.SPSA,\n    beta=0.1,\n    epsilon=0.01,\n)\n\nloss_qng_spsa = []\nfor i in range(n_epochs_qng_spsa):\n    optimizer.zero_grad()\n    loss = mse_loss(model(values=x_train).squeeze(), y_train.squeeze())\n    loss_qng_spsa.append(float(loss))\n    loss.backward()\n    optimizer.step()\n</code></pre>"},{"location":"qinfo_tools/qng/#plotting","title":"Plotting","text":"<p>We now plot the losses corresponding to each of the optimizers: <pre><code># Plot losses\nfig, _ = plt.subplots()\nplt.plot(range(n_epochs_adam), loss_adam, label=\"Adam optimizer\")\nplt.plot(range(n_epochs_qng), loss_qng, label=\"QNG optimizer\")\nplt.plot(range(n_epochs_qng_spsa), loss_qng_spsa, label=\"QNG-SPSA optimizer\")\nplt.legend()\nplt.xlabel(\"Training epochs\")\nplt.ylabel(\"Loss\")\n</code></pre> 2024-08-28T08:01:39.773390 image/svg+xml Matplotlib v3.7.5, https://matplotlib.org/ </p>"},{"location":"qinfo_tools/qng/#references","title":"References","text":"<ol> <li> <p>Meyer J., Information in Noisy Intermediate-Scale Quantum Applications, Quantum 5, 539 (2021) \u21a9</p> </li> <li> <p>Stokes et al., Quantum Natural Gradient, Quantum 4, 269 (2020). \u21a9</p> </li> <li> <p>Gacon et al., Simultaneous Perturbation Stochastic Approximation of the Quantum Fisher Information, Quantum 5, 567 (2021). \u21a9</p> </li> </ol>"}]}